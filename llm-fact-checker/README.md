# ğŸ” LLM Fact-Checking System  
### ğŸ§  Machine Learning Engineer â€“ LLM Task Assignment

A lightweight **LLM-powered Fact Verification System** that extracts claims, retrieves factual evidence, and classifies each claim as **True**, **False**, or **Unverifiable** using a Retrieval-Augmented Generation (RAG) pipeline.

This project is built as per the task instructions from:  
ğŸ“„ *Machine Learning Engineer â€“ LLM Task Assignment* (provided in the challenge)

---

# ğŸŒŸ Features

- âœ‚ï¸ **Automatic Claim Extraction** using NLP  
- ğŸ“š **Trusted Fact Base** ingestion (CSV â†’ chunks â†’ embeddings)  
- ğŸ” **Top-K Retrieval** using FAISS / Chroma  
- ğŸ¤– **LLM-based Comparison** with structured JSON outputs  
- âš¡ Deterministic prompt (temperature = 0) for accuracy  
- ğŸ–¥ï¸ **Streamlit UI** for interactive fact-checking  
- ğŸ§ª Testable pipeline (E2E claim â†’ retrieval â†’ verdict)

---

# ğŸ› ï¸ Tech Stack

| Layer | Tools |
|------|-------|
| **Language** | Python ğŸ |
| **NLP** | spaCy, Transformers |
| **Embeddings** | Sentence-Transformers |
| **Vector DB** | FAISS / Chroma |
| **LLM** | OpenAI GPT-4o / GPT-4o-mini (configurable) |
| **UI** | Streamlit |
| **Data** | Custom facts.csv (trusted dataset) |

---

# ğŸ“‚ Project Structure

```
llm-fact-checker/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ facts.csv
â”‚   â”œâ”€â”€ faiss.index
â”‚   â””â”€â”€ meta.pkl
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ nlp.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ llm_compare.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ demo.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸš€ Getting Started

## 1ï¸âƒ£ Create Virtual Environment
```bash
conda create -n llmFactChecker python
```

---

## 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

---

## 3ï¸âƒ£ Build Vector Index (Embeddings)
```bash
python src/ingest.py
```
This generates:

- `data/faiss.index`  
- `data/meta.pkl`

---

## 4ï¸âƒ£ Run the Fact-Checking App (Streamlit)
```bash
streamlit run src/app.py
```

Open in browser â†’ **http://localhost:8501**

---

# ğŸ§  Pipeline Workflow

### **1. Claim Extraction**
- Uses spaCy  
- Extracts meaningful, verifiable sentences

### **2. Embedding + Retrieval**
- Embeds claims using `all-MiniLM-L6-v2`  
- Retrieves the top-K closest factual statements

### **3. LLM Comparison**
LLM receives:
- Claim  
- Retrieved Evidence  
- JSON-only instructions  

Returns:
```json
{
  "verdict": "True | False | Unverifiable",
  "evidence": [...],
  "reasoning": "Short explanation"
}
```

### **4. Streamlit UI**
- Paste text â†’ get structured verdicts  
- Supports multiple claims  
- Displays:
  - Verdict  
  - Matched Evidence  
  - Reasoning

---

# ğŸ§ª Testing the System

Use the notebook:
```
notebooks/demo.ipynb
```

Test categories:
- âœ” True claims  
- âŒ False claims  
- ğŸ¤· Unverifiable claims  
- ğŸ” Retrieval boundary tests  

---

# ğŸ“¦ Deliverables (As Required)

- ğŸ—‚ï¸ Python code (.py + notebook)  
- ğŸ“ README.md  
- ğŸ“Š Sample test data (input + output)  
- ğŸ¥ 5â€“7 min Video walkthrough  
- ğŸ§ª Example prompts and results  
- ğŸ“ Zip file or GitHub link  

---

# âš ï¸ Limitations

- â— Performance depends on size/quality of **facts.csv**  
- â— LLM can hallucinate â†’ mitigated using strict prompts  
- â— Retrieval errors may mislead the LLM  

---

# ğŸš€ Future Improvements

- ğŸ”„ Better Claim Classification Model (fine-tuned)  
- ğŸ§® Date Reasoning (temporal matching)  
- ğŸ›ï¸ Larger fact database (Gov sources / verified news)  
- ğŸ¤– Local LLM (Mistral / Llama-3) support  
- ğŸ“ˆ Add confidence scores  

---

# ğŸ‘©â€ğŸ’» Author

**Puja Kumari**  
AI/ML Engineer | LLM Developer | Generative AI  
ğŸ“§ Email: **puja02538@gmail.com**  
ğŸŒ GitHub: **github.com/Pujakumari1202**

---

# ğŸ“œ License
MIT License

---

If you want, I can also generate:

âœ… **Colorful Badges**  
âœ… **Project Banner Image**  
âœ… Full **src/ folder code files**  
âœ… A sample **facts.csv (30 entries)**  
Just tell me!
