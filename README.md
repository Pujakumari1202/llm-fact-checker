# ğŸ” LLM Fact-Checking System  
### ğŸ§  Machine Learning Engineer â€“ LLM Task Assignment

A lightweight **LLM-powered Fact Verification System** that takes a claim, retrieves factual evidence, and classifies it as **Likely True**, **Likely False**, or **Unverifiable** using a Retrieval-Augmented Generation (RAG) pipeline.

This project was built for the task described in the provided assignment PDF:  
`file:///mnt/data/Machine Learning Engineer â€“ LLM Task Assignment.pdf`

---

## ğŸŒŸ Features

- ğŸ“ Simple claim input (user-provided)
- ğŸ“š Trusted fact base ingestion (`data/facts.csv`)
- ğŸ” Semantic Top-K retrieval using **ChromaDB**
- ğŸ§  Sentence embeddings via `all-MiniLM-L6-v2`
- ğŸ¤– LLM reasoning & classification via HuggingFace Inference API
- ğŸ“¦ Structured JSON output: `{ "verdict", "evidence", "reasoning" }`
- ğŸ–¥ï¸ Streamlit UI for interactive checks

---


# ğŸ› ï¸ Tech Stack

| Layer | Tools |
|-------|--------|
| **Language** | Python ğŸ |
| **NLP** | spaCy (en_core_web_sm) |
| **Embeddings** | Sentence-Transformers (all-MiniLM-L6-v2) |
| **Vector DB** | ChromaDB |
| **LLM (API)** | HuggingFace Inference API (Mistral-7B-Instruct or any free model) |
| **UI** | Streamlit |
| **Dataset** | Custom `facts.csv` (trusted fact base) |
| **Environment** | Conda + `.env` secrets |
| **Utilities** | pandas, numpy, tqdm |


---


# ğŸ“‚ Project Structure

```
llm-fact-checker/
â”‚
â”œâ”€â”€ llmfact/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ llm_compare.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ facts.csv
â”‚
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

```

---

# ğŸš€ Getting Started

## 1ï¸âƒ£ Create Virtual Environment
```bash
conda create -n llmFactChecker python=3.10 -y


```

---

## 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

---


## 3 Run the Fact-Checking App (Streamlit)
```bash
streamlit run app.py

```

Open in browser â†’ **http://localhost:8501**

---

# ğŸ§  Pipeline Workflow

### **1. Claim Extraction**
- Uses spaCy  
- Extracts meaningful, verifiable sentences

### **2. Embedding + Retrieval**
- Embeds claims using `all-MiniLM-L6-v2`  
- Retrieves the top-K closest factual statements

### **3. LLM Comparison**
LLM receives:
- Claim  
- Retrieved Evidence  
- JSON-only instructions  

Returns:
```json
{
  "verdict": "True | False | Unverifiable",
  "evidence": [...],
  "reasoning": "Short explanation"
}
```

### **4. Streamlit UI**
- Paste text â†’ get structured verdicts  
- Supports multiple claims  
- Displays:
  - Verdict  
  - Matched Evidence  
  - Reasoning

---

# ğŸ§ª Testing the System

Use the notebook:
```
notebooks/demo.ipynb
```

Test categories:
- âœ” True claims  
- âŒ False claims  
- ğŸ¤· Unverifiable claims  
- ğŸ” Retrieval boundary tests  

---

# ğŸ“¦ Deliverables (As Required)

- ğŸ—‚ï¸ Python code (.py + notebook)  
- ğŸ“ README.md  
- ğŸ“Š Sample test data (input + output)  
- ğŸ¥ 5â€“7 min Video walkthrough  
- ğŸ§ª Example prompts and results  
- ğŸ“ Zip file or GitHub link  

---

# âš ï¸ Limitations

- â— Performance depends on size/quality of **facts.csv**  
- â— LLM can hallucinate â†’ mitigated using strict prompts  
- â— Retrieval errors may mislead the LLM  

---

# ğŸš€ Future Improvements

- ğŸ”„ Better Claim Classification Model (fine-tuned)  
- ğŸ§® Date Reasoning (temporal matching)  
- ğŸ›ï¸ Larger fact database (Gov sources / verified news)  
- ğŸ¤– Local LLM (Mistral / Llama-3) support  
- ğŸ“ˆ Add confidence scores  

---

# ğŸ‘©â€ğŸ’» Author

**Puja Kumari**  
AI/ML Engineer | LLM Developer | Generative AI  
ğŸ“§ Email: **puja02538@gmail.com**  
ğŸŒ GitHub: **github.com/Pujakumari1202**

---

# ğŸ“œ License
MIT License

---

If you want, I can also generate:

âœ… **Colorful Badges**  
âœ… **Project Banner Image**  
âœ… Full **src/ folder code files**  
âœ… A sample **facts.csv (30 entries)**  
Just tell me!
